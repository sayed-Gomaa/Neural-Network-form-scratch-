{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwu/u+o3RqJKBbd8sfG944",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayed-Gomaa/Neural-Network-form-scratch-/blob/main/practice%20on%20frist%20DL_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuIlpuZih_9L"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJoqJ-aViZbV"
      },
      "source": [
        "dataset = pd.read_csv('Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hapls3m4iZe_"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "lebel_encode=LabelEncoder()\n",
        "X[:,2]=lebel_encode.fit_transform(X[:,2])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtNFZbEaiZiD"
      },
      "source": [
        "columnTransformer=ColumnTransformer([('encoder',OneHotEncoder(),[1])],remainder='passthrough')\n",
        "X = np.array(columnTransformer.fit_transform(X), dtype = np.str)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_5hNGDCiZk_"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqqh0CuZiZnR"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUR-i71TiZqW"
      },
      "source": [
        "def sigmoid(Z):\n",
        "     \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "     \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDY-6mkmiZtN"
      },
      "source": [
        "def init_params(layer_dims):\n",
        "  params={}\n",
        "  L=len(layer_dims)\n",
        "  for l in range(1,L):\n",
        "    params['W'+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])\n",
        "    params['b'+str(l)]=np.zeros((layer_dims[l],1))\n",
        "  return params"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZBNbzuPiZwF"
      },
      "source": [
        "def feedfworward(X, paramters):\n",
        "   \n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(paramters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "  \n",
        "    for l in range(1, L):\n",
        "\n",
        "        A_prev = A \n",
        "        Z=np.dot(paramters['W'+str(l)],A_prev)+paramters['b'+str(l)]\n",
        "        A,activation_cache=relu(Z)\n",
        "        \n",
        "        linear_cache = (A_prev, paramters['W'+str(l)], paramters['b'+str(l)])\n",
        "        cache=(linear_cache,activation_cache)\n",
        "        caches.append(cache)\n",
        "        \n",
        "    #output layer\n",
        "    Z=np.dot(paramters['W'+str(L)],A)+paramters['b'+str(L)]\n",
        "    AL,activation_cache=sigmoid(Z)\n",
        "    linear_cache = (A, paramters['W'+str(L)], paramters['b'+str(L)])\n",
        "    cache=(linear_cache,activation_cache)\n",
        "    caches.append(cache)\n",
        "      \n",
        "    return AL, caches"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXXsJJhvmKwU"
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = len(Y)\n",
        "    \n",
        "    cost= -1/m * np.sum(np.dot(Y,np.log(AL).T) + np.dot((1-Y),np.log(1-AL).T) )\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvXggoDaiZzB"
      },
      "source": [
        " def feedbackward(AL, Y, caches):\n",
        "    \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "     \n",
        "    # Initializing the backpropagation\n",
        "    dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
        "     \n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. \n",
        "    \n",
        "    linear_cache, activation_cache = caches[L-1]\n",
        "    dZ = sigmoid_backward(dAL, activation_cache) \n",
        "    A_prev, W, b = linear_cache\n",
        "     \n",
        "    m = len(A_prev)\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db =1/m * np.sum (dZ, axis=1,keepdims=True)\n",
        "    dA_prev = np.dot( W.T, dZ)\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev\n",
        "    grads[\"dW\" + str(L)] = dW\n",
        "    grads[\"db\" + str(L)] = db \n",
        "    \n",
        "\n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        linear_cache, activation_cache = caches[l]\n",
        "        dZ  = relu_backward(grads[\"dA\" + str(l + 1)], activation_cache) \n",
        "        \n",
        "        A_prev, W, b = linear_cache\n",
        "        m = len(A_prev)\n",
        "        \n",
        "        dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "        db =1/m * np.sum (dZ, axis=1,keepdims=True)\n",
        "        dA_prev = np.dot( W.T, dZ)\n",
        "        \n",
        "        grads[\"dA\" + str(l)] = dA_prev\n",
        "        grads[\"dW\" + str(l + 1)] = dW\n",
        "        grads[\"db\" + str(l + 1)] = db\n",
        "      \n",
        "\n",
        "        \n",
        "    return grads"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxK1d48di1K7"
      },
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "    parameters = params.copy()\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        " \n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate *grads[\"db\" + str(l+1)]\n",
        "       \n",
        "    return parameters"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXF2hgXTi1Ob"
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    costs = []                              \n",
        "    parameters = init_params(layers_dims)\n",
        "    \n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "        AL, caches =feedfworward(X, parameters)\n",
        "         \n",
        "        # Compute cost.\n",
        "        cost =  compute_cost(AL, Y)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = feedbackward(AL, Y, caches)\n",
        "       \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 iterations\n",
        "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if i % 100 == 0 or i == num_iterations:\n",
        "            costs.append(cost)\n",
        "    \n",
        "    return parameters, costs"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z22WDPfsi1pS",
        "outputId": "51d23b88-55a8-4b9a-a9df-43a8e29d52ea"
      },
      "source": [
        "layers_dims = [12, 5, 1] #  3-layer model\n",
        "parameters, costs = L_layer_model(X_train.T,y_train, layers_dims, num_iterations = 1000, print_cost = True)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 1.0815913265877184\n",
            "Cost after iteration 100: 0.4138278476122523\n",
            "Cost after iteration 200: 0.40274553546648223\n",
            "Cost after iteration 300: 0.39968918966861355\n",
            "Cost after iteration 400: 0.3981306203405105\n",
            "Cost after iteration 500: 0.3965494217587824\n",
            "Cost after iteration 600: 0.3965938347603027\n",
            "Cost after iteration 700: 0.3966800714386534\n",
            "Cost after iteration 800: 0.395271187840922\n",
            "Cost after iteration 900: 0.39522767435385325\n",
            "Cost after iteration 999: 0.39962184946753415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvITZ-yujpwK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}